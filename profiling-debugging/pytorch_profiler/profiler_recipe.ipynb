{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Profiler\n",
    "================\n",
    "\n",
    "This recipe explains how to use PyTorch profiler and measure the time\n",
    "and memory consumption of the model\\'s operators.\n",
    "\n",
    "Introduction\n",
    "------------\n",
    "\n",
    "PyTorch includes a simple profiler API that is useful when user needs to\n",
    "determine the most expensive operators in the model.\n",
    "\n",
    "In this recipe, we will use a simple Resnet model to demonstrate how to\n",
    "use profiler to analyze model performance.\n",
    "\n",
    "Setup\n",
    "-----\n",
    "\n",
    "To install `torch` and `torchvision` use the following command:\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "pip install torch torchvision\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps\n",
    "=====\n",
    "\n",
    "1.  Import all necessary libraries\n",
    "2.  Instantiate a simple Resnet model\n",
    "3.  Using profiler to analyze execution time\n",
    "4.  Using profiler to analyze memory consumption\n",
    "5.  Using tracing functionality\n",
    "6.  Examining stack traces\n",
    "7.  Using profiler to analyze long-running jobs\n",
    "\n",
    "1. Import all necessary libraries\n",
    "---------------------------------\n",
    "\n",
    "In this recipe we will use `torch`, `torchvision.models` and `profiler`\n",
    "modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Instantiate a simple Resnet model\n",
    "====================================\n",
    "\n",
    "Let\\'s create an instance of a Resnet model and prepare an input for it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Using profiler to analyze execution time\n",
    "===========================================\n",
    "\n",
    "PyTorch profiler is enabled through the context manager and accepts a\n",
    "number of parameters, some of the most useful are:\n",
    "\n",
    "-   \n",
    "\n",
    "    `activities` - a list of activities to profile:\n",
    "\n",
    "    :   -   `ProfilerActivity.CPU` - PyTorch operators, TorchScript\n",
    "            functions and user-defined code labels (see\n",
    "            `record_function` below);\n",
    "        -   `ProfilerActivity.CUDA` - on-device CUDA kernels;\n",
    "        -   `ProfilerActivity.XPU` - on-device XPU kernels;\n",
    "\n",
    "-   `record_shapes` - whether to record shapes of the operator inputs;\n",
    "-   `profile_memory` - whether to report amount of memory consumed by\n",
    "    model\\'s Tensors;\n",
    "\n",
    "Note: when using CUDA, profiler also shows the runtime CUDA events\n",
    "occurring on the host.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s see how we can use profiler to analyze the execution time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:00:41 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-25 12:00:41 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:00:41 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use `record_function` context manager to label\n",
    "arbitrary code ranges with user provided names (`model_inference` is\n",
    "used as a label in the example above).\n",
    "\n",
    "Profiler allows one to check which operators were called during the\n",
    "execution of a code range wrapped with a profiler context manager. If\n",
    "multiple profiler ranges are active at the same time (e.g. in parallel\n",
    "PyTorch threads), each profiling context manager tracks only the\n",
    "operators of its corresponding range. Profiler also automatically\n",
    "profiles the asynchronous tasks launched with `torch.jit._fork` and (in\n",
    "case of a backward pass) the backward pass operators launched with\n",
    "`backward()` call.\n",
    "\n",
    "Let\\'s print out the stats for the execution above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         4.64%       8.857ms       100.00%     190.862ms     190.862ms             1  \n",
      "                     aten::conv2d         0.31%     591.000us        65.13%     124.313ms       6.216ms            20  \n",
      "                aten::convolution         1.26%       2.399ms        64.82%     123.722ms       6.186ms            20  \n",
      "               aten::_convolution         0.40%     761.000us        63.57%     121.323ms       6.066ms            20  \n",
      "         aten::mkldnn_convolution        62.81%     119.880ms        63.17%     120.562ms       6.028ms            20  \n",
      "                 aten::batch_norm         0.03%      62.000us        10.94%      20.888ms       1.044ms            20  \n",
      "     aten::_batch_norm_impl_index         0.40%     767.000us        10.91%      20.826ms       1.041ms            20  \n",
      "          aten::native_batch_norm        10.45%      19.936ms        10.50%      20.042ms       1.002ms            20  \n",
      "                 aten::max_pool2d         0.01%      15.000us         6.99%      13.332ms      13.332ms             1  \n",
      "    aten::max_pool2d_with_indices         6.98%      13.317ms         6.98%      13.317ms      13.317ms             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 190.862ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will look like (omitting some columns):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------  ------------  ------------  ------------  ------------\n",
    "#                              Name      Self CPU     CPU total  CPU time avg    # of Calls\n",
    "# ---------------------------------  ------------  ------------  ------------  ------------\n",
    "#                   model_inference       5.509ms      57.503ms      57.503ms             1\n",
    "#                      aten::conv2d     231.000us      31.931ms       1.597ms            20\n",
    "#                 aten::convolution     250.000us      31.700ms       1.585ms            20\n",
    "#                aten::_convolution     336.000us      31.450ms       1.573ms            20\n",
    "#          aten::mkldnn_convolution      30.838ms      31.114ms       1.556ms            20\n",
    "#                  aten::batch_norm     211.000us      14.693ms     734.650us            20\n",
    "#      aten::_batch_norm_impl_index     319.000us      14.482ms     724.100us            20\n",
    "#           aten::native_batch_norm       9.229ms      14.109ms     705.450us            20\n",
    "#                        aten::mean     332.000us       2.631ms     125.286us            21\n",
    "#                      aten::select       1.668ms       2.292ms       8.988us           255\n",
    "# ---------------------------------  ------------  ------------  ------------  ------------\n",
    "# Self CPU time total: 57.549m\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that, as expected, most of the time is spent in convolution\n",
    "(and specifically in `mkldnn_convolution` for PyTorch compiled with\n",
    "`MKL-DNN` support). Note the difference between self cpu time and cpu\n",
    "time - operators can call other operators, self cpu time excludes time\n",
    "spent in children operator calls, while total cpu time includes it. You\n",
    "can choose to sort by the self cpu time by passing\n",
    "`sort_by=\"self_cpu_time_total\"` into the `table` call.\n",
    "\n",
    "To get a finer granularity of results and include operator input shapes,\n",
    "pass `group_by_input_shape=True` (note: this requires running the\n",
    "profiler with `record_shapes=True`):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         4.64%       8.857ms       100.00%     190.862ms     190.862ms             1                                                                                []  \n",
      "                     aten::conv2d         0.26%     493.000us        31.70%      60.512ms      60.512ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "                aten::convolution         1.13%       2.151ms        31.45%      60.019ms      60.019ms             1                     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.31%     593.000us        30.32%      57.868ms      57.868ms             1     [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        29.76%      56.794ms        30.01%      57.275ms      57.275ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.01%      25.000us         9.19%      17.539ms       4.385ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.04%      70.000us         9.18%      17.514ms       4.378ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.03%      49.000us         9.14%      17.444ms       4.361ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution         9.09%      17.346ms         9.11%      17.395ms       4.349ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                 aten::max_pool2d         0.01%      15.000us         6.99%      13.332ms      13.332ms             1                                           [[5, 64, 112, 112], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 190.862ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might look like this (omitting some columns):\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "---------------------------------  ------------  -------------------------------------------\n",
    "                             Name     CPU total                                 Input Shapes\n",
    "---------------------------------  ------------  -------------------------------------------\n",
    "                  model_inference      57.503ms                                           []\n",
    "                     aten::conv2d       8.008ms      [5,64,56,56], [64,64,3,3], [], ..., []]\n",
    "                aten::convolution       7.956ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n",
    "               aten::_convolution       7.909ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n",
    "         aten::mkldnn_convolution       7.834ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n",
    "                     aten::conv2d       6.332ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n",
    "                aten::convolution       6.303ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n",
    "               aten::_convolution       6.273ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n",
    "         aten::mkldnn_convolution       6.233ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n",
    "                     aten::conv2d       4.751ms  [[5,256,14,14], [256,256,3,3], [], ..., []]\n",
    "---------------------------------  ------------  -------------------------------------------\n",
    "Self CPU time total: 57.549ms\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the occurrence of `aten::convolution` twice with different input\n",
    "shapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiler can also be used to analyze performance of models executed on\n",
    "GPUs and XPUs: Users could switch between cpu, cuda and xpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:02:55 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         0.43%       2.653ms       100.00%     611.339ms     611.339ms       0.000us         0.00%       4.214ms       4.214ms             1  \n",
      "                                           aten::conv2d         0.01%      83.000us        81.37%     497.440ms      24.872ms       0.000us         0.00%       3.527ms     176.350us            20  \n",
      "                                      aten::convolution         0.04%     237.000us        81.35%     497.357ms      24.868ms       0.000us         0.00%       3.527ms     176.350us            20  \n",
      "                                     aten::_convolution         0.16%     990.000us        81.32%     497.120ms      24.856ms       0.000us         0.00%       3.527ms     176.350us            20  \n",
      "                                aten::cudnn_convolution        61.37%     375.180ms        81.15%     496.130ms      24.806ms       3.000ms        81.37%       3.527ms     176.350us            20  \n",
      "cudnn_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_t...         0.00%       0.000us         0.00%       0.000us       0.000us       1.303ms        35.34%       1.303ms     130.300us            10  \n",
      "cudnn_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_sm...         0.00%       0.000us         0.00%       0.000us       0.000us       1.001ms        27.15%       1.001ms     200.200us             5  \n",
      "                                       aten::batch_norm         0.01%      45.000us         4.08%      24.964ms       1.248ms       0.000us         0.00%     356.000us      17.800us            20  \n",
      "                           aten::_batch_norm_impl_index         0.02%     104.000us         4.08%      24.919ms       1.246ms       0.000us         0.00%     356.000us      17.800us            20  \n",
      "                                 aten::cudnn_batch_norm         1.91%      11.655ms         4.06%      24.815ms       1.241ms     356.000us         9.66%     356.000us      17.800us            20  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 611.348ms\n",
      "Self CUDA time total: 3.687ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:02:56 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:02:56 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.xpu.is_available():\n",
    "    device = 'xpu'\n",
    "else:\n",
    "    print('Neither CUDA nor XPU devices are available to demonstrate profiling on acceleration devices')\n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\n",
    "sort_by_keyword = device + \"_time_total\"\n",
    "\n",
    "model = models.resnet18().to(device)\n",
    "inputs = torch.randn(5, 3, 224, 224).to(device)\n",
    "\n",
    "with profile(activities=activities, record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=sort_by_keyword, row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: the first use of CUDA profiling may bring an extra overhead.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting table output (omitting some columns):\n",
    "\n",
    "> ``` {.sourceCode .sh}\n",
    "> -------------------------------------------------------  ------------  ------------\n",
    ">                                                    Name     Self CUDA    CUDA total\n",
    "> -------------------------------------------------------  ------------  ------------\n",
    ">                                         model_inference       0.000us      11.666ms\n",
    ">                                            aten::conv2d       0.000us      10.484ms\n",
    ">                                       aten::convolution       0.000us      10.484ms\n",
    ">                                      aten::_convolution       0.000us      10.484ms\n",
    ">                              aten::_convolution_nogroup       0.000us      10.484ms\n",
    ">                                       aten::thnn_conv2d       0.000us      10.484ms\n",
    ">                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n",
    "> void at::native::im2col_kernel<float>(long, float co...       3.844ms       3.844ms\n",
    ">                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n",
    ">                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n",
    "> -------------------------------------------------------  ------------  ------------\n",
    "> Self CPU time total: 23.015ms\n",
    "> Self CUDA time total: 11.666ms\n",
    "> ```\n",
    "\n",
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: the first use of XPU profiling may bring an extra overhead.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting table output (omitting some columns):\n",
    "\n",
    "> ``` {.sourceCode .sh}\n",
    "> ```\n",
    "\n",
    "\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- Name Self XPU Self XPU % XPU total XPU time avg \\# of Calls\n",
    "\n",
    ":   \n",
    "\n",
    "    \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- model\\_inference 0.000us 0.00% 2.567ms 2.567ms 1\n",
    "\n",
    "    :   aten::conv2d 0.000us 0.00% 1.871ms 93.560us 20\n",
    "\n",
    "    aten::convolution 0.000us 0.00% 1.871ms 93.560us 20\n",
    "\n",
    "    :   aten::\\_convolution 0.000us 0.00% 1.871ms 93.560us 20\n",
    "\n",
    "    aten::convolution\\_overrideable 1.871ms 72.89% 1.871ms 93.560us 20\n",
    "\n",
    "    :   gen\\_conv 1.484ms 57.82% 1.484ms 74.216us 20\n",
    "\n",
    "    aten::batch\\_norm 0.000us 0.00% 432.640us 21.632us 20\n",
    "\n",
    "    :   \n",
    "\n",
    "        aten::\\_batch\\_norm\\_impl\\_index 0.000us 0.00% 432.640us 21.632us 20\n",
    "\n",
    "        :   \n",
    "\n",
    "            aten::native\\_batch\\_norm 432.640us 16.85% 432.640us 21.632us 20\n",
    "\n",
    "            :   conv\\_reorder 386.880us 15.07% 386.880us 6.448us 60\n",
    "\n",
    "    \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n",
    "    \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n",
    "    \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\-- \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--\n",
    "    \\-\\-\\-\\-\\-\\-\\-\\-\\-\\-\\--Self CPU time total: 712.486ms Self XPU time\n",
    "    total: 2.567ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the occurrence of on-device kernels in the output (e.g.\n",
    "`sgemm_32x32x32_NN`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using profiler to analyze memory consumption\n",
    "===============================================\n",
    "\n",
    "PyTorch profiler can also show the amount of memory (used by the\n",
    "model\\'s tensors) that was allocated (or released) during the execution\n",
    "of the model\\'s operators. In the output below, \\'self\\' memory\n",
    "corresponds to the memory allocated (released) by the operator,\n",
    "excluding the children calls to the other operators. To enable memory\n",
    "profiling functionality pass `profile_memory=True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.53%     326.000us         0.53%     326.000us       1.630us      90.54 Mb      90.54 Mb           200  \n",
      "    aten::max_pool2d_with_indices         8.42%       5.210ms         8.42%       5.210ms       5.210ms      11.48 Mb      11.48 Mb             1  \n",
      "                 aten::empty_like         0.10%      59.000us         0.17%     105.000us       5.250us      47.37 Mb       4.31 Mb            20  \n",
      "                      aten::addmm         0.23%     141.000us         0.25%     157.000us     157.000us      19.53 Kb      19.53 Kb             1  \n",
      "                       aten::mean         0.03%      19.000us         0.16%     102.000us     102.000us      10.00 Kb      10.00 Kb             1  \n",
      "              aten::empty_strided         0.01%       4.000us         0.01%       4.000us       4.000us           4 b           4 b             1  \n",
      "                     aten::conv2d         0.16%      96.000us        72.80%      45.032ms       2.252ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.43%     264.000us        72.64%      44.936ms       2.247ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.28%     172.000us        72.22%      44.672ms       2.234ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        71.55%      44.259ms        71.94%      44.500ms       2.225ms      47.37 Mb           0 b            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 61.858ms\n",
      "\n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.53%     326.000us         0.53%     326.000us       1.630us      90.54 Mb      90.54 Mb           200  \n",
      "                 aten::batch_norm         0.11%      67.000us        16.36%      10.123ms     506.150us      47.41 Mb           0 b            20  \n",
      "     aten::_batch_norm_impl_index         0.21%     132.000us        16.26%      10.056ms     502.800us      47.41 Mb           0 b            20  \n",
      "          aten::native_batch_norm        15.68%       9.698ms        16.01%       9.902ms     495.100us      47.41 Mb     -64.75 Kb            20  \n",
      "                     aten::conv2d         0.16%      96.000us        72.80%      45.032ms       2.252ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.43%     264.000us        72.64%      44.936ms       2.247ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.28%     172.000us        72.22%      44.672ms       2.234ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        71.55%      44.259ms        71.94%      44.500ms       2.225ms      47.37 Mb           0 b            20  \n",
      "                 aten::empty_like         0.10%      59.000us         0.17%     105.000us       5.250us      47.37 Mb       4.31 Mb            20  \n",
      "                 aten::max_pool2d         0.01%       6.000us         8.43%       5.216ms       5.216ms      11.48 Mb           0 b             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 61.858ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:03:46 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-25 12:03:46 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:03:46 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "\n",
    "# (omitting some columns)\n",
    "# ---------------------------------  ------------  ------------  ------------\n",
    "#                              Name       CPU Mem  Self CPU Mem    # of Calls\n",
    "# ---------------------------------  ------------  ------------  ------------\n",
    "#                       aten::empty      94.79 Mb      94.79 Mb           121\n",
    "#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n",
    "#                       aten::addmm      19.53 Kb      19.53 Kb             1\n",
    "#               aten::empty_strided         572 b         572 b            25\n",
    "#                     aten::resize_         240 b         240 b             6\n",
    "#                         aten::abs         480 b         240 b             4\n",
    "#                         aten::add         160 b         160 b            20\n",
    "#               aten::masked_select         120 b         112 b             1\n",
    "#                          aten::ne         122 b          53 b             6\n",
    "#                          aten::eq          60 b          30 b             2\n",
    "# ---------------------------------  ------------  ------------  ------------\n",
    "# Self CPU time total: 53.064ms\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::conv2d         1.22%     143.000us        28.82%       3.391ms     169.550us       0.000us         0.00%       7.624ms     381.200us           0 b           0 b      49.18 Mb       1.91 Mb            20  \n",
      "                                      aten::convolution         1.82%     214.000us        28.12%       3.308ms     165.400us       0.000us         0.00%       7.880ms     394.000us           0 b           0 b      49.18 Mb           0 b            20  \n",
      "                                     aten::_convolution         1.21%     142.000us        26.30%       3.094ms     154.700us       0.000us         0.00%       7.880ms     394.000us           0 b           0 b      49.18 Mb           0 b            20  \n",
      "                                aten::cudnn_convolution        14.29%       1.681ms        25.09%       2.952ms     147.600us       7.880ms        82.63%       7.880ms     394.000us           0 b           0 b      49.18 Mb      49.18 Mb            20  \n",
      "                                       aten::batch_norm         0.36%      42.000us        13.51%       1.590ms      79.500us       0.000us         0.00%     779.000us      38.950us           0 b           0 b      48.27 Mb           0 b            20  \n",
      "                           aten::_batch_norm_impl_index         0.65%      76.000us        13.16%       1.548ms      77.400us       0.000us         0.00%     779.000us      38.950us           0 b           0 b      48.27 Mb           0 b            20  \n",
      "                                 aten::cudnn_batch_norm         6.89%     811.000us        12.51%       1.472ms      73.600us     779.000us         8.17%     779.000us      38.950us           0 b           0 b      48.27 Mb           0 b            20  \n",
      "                                            aten::empty         3.36%     395.000us         3.36%     395.000us       3.950us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      48.27 Mb      48.27 Mb           100  \n",
      "                                       aten::empty_like         0.60%      71.000us         1.63%     192.000us       9.600us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      48.23 Mb           0 b            20  \n",
      "                                       aten::max_pool2d         0.05%       6.000us         0.31%      37.000us      37.000us       0.000us         0.00%     347.000us     347.000us           0 b           0 b      12.16 Mb           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.765ms\n",
      "Self CUDA time total: 9.537ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:07:58 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-25 12:07:58 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:07:58 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.xpu.is_available():\n",
    "    device = 'xpu'\n",
    "else:\n",
    "    print('Neither CUDA nor XPU devices are available to demonstrate profiling on acceleration devices')\n",
    "    import sys\n",
    "    sys.exit(0)\n",
    "\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\n",
    "#sort_by_keyword = device + \"_time_total\"\n",
    "sort_by_keyword = device + \"_memory_usage\"\n",
    "\n",
    "model = models.resnet18().to(device)\n",
    "inputs = torch.randn(5, 3, 224, 224).to(device)\n",
    "\n",
    "with profile(activities=activities, profile_memory=True,  record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=sort_by_keyword, row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might look like this (omitting some columns):\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "---------------------------------  ------------  ------------  ------------\n",
    "                             Name       CPU Mem  Self CPU Mem    # of Calls\n",
    "---------------------------------  ------------  ------------  ------------\n",
    "                      aten::empty      94.79 Mb      94.79 Mb           121\n",
    "                 aten::batch_norm      47.41 Mb           0 b            20\n",
    "     aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n",
    "          aten::native_batch_norm      47.41 Mb           0 b            20\n",
    "                     aten::conv2d      47.37 Mb           0 b            20\n",
    "                aten::convolution      47.37 Mb           0 b            20\n",
    "               aten::_convolution      47.37 Mb           0 b            20\n",
    "         aten::mkldnn_convolution      47.37 Mb           0 b            20\n",
    "                 aten::max_pool2d      11.48 Mb           0 b             1\n",
    "    aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n",
    "---------------------------------  ------------  ------------  ------------\n",
    "Self CPU time total: 53.064ms\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Using tracing functionality\n",
    "==============================\n",
    "\n",
    "Profiling results can be outputted as a `.json` trace file: Tracing CUDA\n",
    "or XPU kernels Users could switch between cpu, cuda and xpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:08:12 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-25 12:08:12 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:08:12 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA, ProfilerActivity.XPU]\n",
    "\n",
    "model = models.resnet18().to(device)\n",
    "inputs = torch.randn(5, 3, 224, 224).to(device)\n",
    "\n",
    "with profile(activities=activities) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the sequence of profiled operators and CUDA/XPU kernels\n",
    "in Chrome trace viewer (`chrome://tracing`):\n",
    "\n",
    "![image](https://pytorch.org/tutorials/_static/img/trace_img.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Examining stack traces\n",
    "=========================\n",
    "\n",
    "Profiler can be used to analyze Python and TorchScript stack traces:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::cudnn_convolution         5.19%       1.558ms        77.84%      23.378ms       1.169ms       7.784ms        81.82%       7.784ms     389.200us            20  \n",
      "cudnn_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_t...         0.00%       0.000us         0.00%       0.000us       0.000us       4.402ms        46.27%       4.402ms     440.200us            10  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 30.033ms\n",
      "Self CUDA time total: 9.513ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:08:54 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-25 12:08:54 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:08:54 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "sort_by_keyword = \"self_\" + device + \"_time_total\"\n",
    "\n",
    "with profile(\n",
    "    activities=activities,\n",
    "    with_stack=True,\n",
    ") as prof:\n",
    "    model(inputs)\n",
    "\n",
    "# Print aggregated stats\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=sort_by_keyword, row_limit=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output might look like this (omitting some columns):\n",
    "\n",
    "``` {.sourceCode .sh}\n",
    "-------------------------  -----------------------------------------------------------\n",
    "                     Name  Source Location\n",
    "-------------------------  -----------------------------------------------------------\n",
    "aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n",
    "                           .../torch/nn/modules/conv.py(443): forward\n",
    "                           .../torch/nn/modules/module.py(1051): _call_impl\n",
    "                           .../site-packages/torchvision/models/resnet.py(63): forward\n",
    "                           .../torch/nn/modules/module.py(1051): _call_impl\n",
    "aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n",
    "                           .../torch/nn/modules/conv.py(443): forward\n",
    "                           .../torch/nn/modules/module.py(1051): _call_impl\n",
    "                           .../site-packages/torchvision/models/resnet.py(59): forward\n",
    "                           .../torch/nn/modules/module.py(1051): _call_impl\n",
    "-------------------------  -----------------------------------------------------------\n",
    "Self CPU time total: 34.016ms\n",
    "Self CUDA time total: 11.659ms\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two convolutions and the two call sites in\n",
    "`torchvision/models/resnet.py` script.\n",
    "\n",
    "(Warning: stack tracing adds an extra profiling overhead.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Using profiler to analyze long-running jobs\n",
    "==============================================\n",
    "\n",
    "PyTorch profiler offers an additional API to handle long-running jobs\n",
    "(such as training loops). Tracing all of the execution can be slow and\n",
    "result in very large trace files. To avoid this, use optional arguments:\n",
    "\n",
    "-   `schedule` - specifies a function that takes an integer argument\n",
    "    (step number) as an input and returns an action for the profiler,\n",
    "    the best way to use this parameter is to use\n",
    "    `torch.profiler.schedule` helper function that can generate a\n",
    "    schedule for you;\n",
    "-   `on_trace_ready` - specifies a function that takes a reference to\n",
    "    the profiler as an input and is called by the profiler each time the\n",
    "    new trace is ready.\n",
    "\n",
    "To illustrate how the API works, let\\'s first consider the following\n",
    "example with `torch.profiler.schedule` helper function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.profiler import schedule\n",
    "\n",
    "my_schedule = schedule(\n",
    "    skip_first=10,\n",
    "    wait=5,\n",
    "    warmup=1,\n",
    "    active=3,\n",
    "    repeat=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiler assumes that the long-running job is composed of steps,\n",
    "numbered starting from zero. The example above defines the following\n",
    "sequence of actions for the profiler:\n",
    "\n",
    "1.  Parameter `skip_first` tells profiler that it should ignore the\n",
    "    first 10 steps (default value of `skip_first` is zero);\n",
    "2.  After the first `skip_first` steps, profiler starts executing\n",
    "    profiler cycles;\n",
    "3.  Each cycle consists of three phases:\n",
    "    -   idling (`wait=5` steps), during this phase profiler is not\n",
    "        active;\n",
    "    -   warming up (`warmup=1` steps), during this phase profiler starts\n",
    "        tracing, but the results are discarded; this phase is used to\n",
    "        discard the samples obtained by the profiler at the beginning of\n",
    "        the trace since they are usually skewed by an extra overhead;\n",
    "    -   active tracing (`active=3` steps), during this phase profiler\n",
    "        traces and records data;\n",
    "4.  An optional `repeat` parameter specifies an upper bound on the\n",
    "    number of cycles. By default (zero value), profiler will execute\n",
    "    cycles as long as the job runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in the example above, profiler will skip the first 15 steps, spend\n",
    "the next step on the warm up, actively record the next 3 steps, skip\n",
    "another 5 steps, spend the next step on the warm up, actively record\n",
    "another 3 steps. Since the `repeat=2` parameter value is specified, the\n",
    "profiler will stop the recording after the first two cycles.\n",
    "\n",
    "At the end of each cycle profiler calls the specified `on_trace_ready`\n",
    "function and passes itself as an argument. This function is used to\n",
    "process the new trace - either by obtaining the table output or by\n",
    "saving the output on disk as a trace file.\n",
    "\n",
    "To send the signal to the profiler that the next step has started, call\n",
    "`prof.step()` function. The current profiler step is stored in\n",
    "`prof.step_num`.\n",
    "\n",
    "The following example shows how to use all of the concepts above for\n",
    "CUDA and XPU Kernels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-10-25 12:09:30 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-25 12:09:30 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:09:30 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2024-10-25 12:09:31 96037:96037 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-10-25 12:09:31 96037:96037 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-10-25 12:09:31 96037:96037 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::cudnn_convolution         9.22%       1.435ms        13.14%       2.044ms      51.100us       8.343ms        54.99%       8.343ms     208.575us            40  \n",
      "cudnn_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_t...         0.00%       0.000us         0.00%       0.000us       0.000us       5.807ms        38.27%       5.807ms     223.346us            26  \n",
      "cudnn_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_sm...         0.00%       0.000us         0.00%       0.000us       0.000us       4.459ms        29.39%       4.459ms     297.267us            15  \n",
      "                                 aten::cudnn_batch_norm         8.48%       1.319ms        15.81%       2.459ms      61.475us     909.000us         5.99%     909.000us      22.725us            40  \n",
      "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     869.000us         5.73%     869.000us      28.967us            30  \n",
      "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     750.000us         4.94%     750.000us      17.045us            44  \n",
      "       cudnn_volta_scudnn_128x64_relu_xregs_large_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     484.000us         3.19%     484.000us     242.000us             2  \n",
      "           cudnn_volta_scudnn_128x128_relu_medium_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     414.000us         2.73%     414.000us     207.000us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     411.000us         2.71%     411.000us       8.935us            46  \n",
      "                                       aten::clamp_min_         1.57%     244.000us         2.83%     440.000us      12.941us     332.000us         2.19%     332.000us       9.765us            34  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 15.556ms\n",
      "Self CUDA time total: 15.173ms\n",
      "\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::cudnn_convolution        14.29%       1.438ms        20.33%       2.045ms      51.125us       5.987ms        80.73%       5.987ms     149.675us            40  \n",
      "cudnn_volta_scudnn_winograd_128x128_ldg1_ldg4_relu_t...         0.00%       0.000us         0.00%       0.000us       0.000us       2.611ms        35.21%       2.611ms     130.550us            20  \n",
      "cudnn_volta_scudnn_128x32_sliced1x4_ldg4_relu_exp_sm...         0.00%       0.000us         0.00%       0.000us       0.000us       1.994ms        26.89%       1.994ms     199.400us            10  \n",
      "                                 aten::cudnn_batch_norm        12.91%       1.299ms        24.53%       2.468ms      61.700us     719.000us         9.70%     719.000us      17.975us            40  \n",
      "void cudnn::ops::nchwToNhwcKernel<float, float, floa...         0.00%       0.000us         0.00%       0.000us       0.000us     413.000us         5.57%     413.000us      20.650us            20  \n",
      "void cudnn::bn_fw_tr_1C11_singleread<float, 512, tru...         0.00%       0.000us         0.00%       0.000us       0.000us     348.000us         4.69%     348.000us      11.226us            31  \n",
      "       cudnn_volta_scudnn_128x64_relu_xregs_large_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     298.000us         4.02%     298.000us     149.000us             2  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     266.000us         3.59%     266.000us       7.600us            35  \n",
      "                                       aten::clamp_min_         2.51%     253.000us         4.49%     452.000us      13.294us     263.000us         3.55%     263.000us       7.735us            34  \n",
      "           cudnn_volta_scudnn_128x128_relu_medium_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us     254.000us         3.43%     254.000us     127.000us             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 10.060ms\n",
      "Self CUDA time total: 7.416ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_by_keyword = \"self_\" + device + \"_time_total\"\n",
    "\n",
    "def trace_handler(p):\n",
    "    output = p.key_averages().table(sort_by=sort_by_keyword, row_limit=10)\n",
    "    print(output)\n",
    "    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n",
    "\n",
    "with profile(\n",
    "    activities=activities,\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=2),\n",
    "    on_trace_ready=trace_handler\n",
    ") as p:\n",
    "    for idx in range(8):\n",
    "        model(inputs)\n",
    "        p.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn More\n",
    "==========\n",
    "\n",
    "Take a look at the following recipes/tutorials to continue your\n",
    "learning:\n",
    "\n",
    "-   [PyTorch\n",
    "    Benchmark](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)\n",
    "-   [PyTorch Profiler with\n",
    "    TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)\n",
    "    tutorial\n",
    "-   [Visualizing models, data, and training with\n",
    "    TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
    "    tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
