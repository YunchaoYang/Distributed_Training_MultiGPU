[2024-07-10 15:12:05,271] torch.distributed.run: [WARNING] 
[2024-07-10 15:12:05,271] torch.distributed.run: [WARNING] *****************************************
[2024-07-10 15:12:05,271] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-07-10 15:12:05,271] torch.distributed.run: [WARNING] *****************************************
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
/red/ufhpc/hityangsir/.conda/envs/Llama2_HF/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for wikihow contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikihow
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
r0 Training Epoch:   0%|[34m          [0m| 0/47 [00:00<?, ?it/s]r0 Training Epoch:   2%|[34m▏         [0m| 1/47 [00:13<10:25, 13.59s/it]r0 Training Epoch:   4%|[34m▍         [0m| 2/47 [00:13<04:16,  5.69s/it]r0 Training Epoch:   6%|[34m▋         [0m| 3/47 [00:13<02:18,  3.16s/it]r0 Training Epoch:   9%|[34m▊         [0m| 4/47 [00:14<01:24,  1.97s/it]r0 Training Epoch:  11%|[34m█         [0m| 5/47 [00:14<00:54,  1.31s/it]r0 Training Epoch:  13%|[34m█▎        [0m| 6/47 [00:14<00:37,  1.10it/s]r0 Training Epoch:  15%|[34m█▍        [0m| 7/47 [00:14<00:26,  1.53it/s]r0 Training Epoch:  17%|[34m█▋        [0m| 8/47 [00:14<00:19,  2.05it/s]r0 Training Epoch:  19%|[34m█▉        [0m| 9/47 [00:14<00:14,  2.64it/s]r0 Training Epoch:  21%|[34m██▏       [0m| 10/47 [00:14<00:11,  3.28it/s]r0 Training Epoch:  23%|[34m██▎       [0m| 11/47 [00:14<00:09,  3.92it/s]r0 Training Epoch:  26%|[34m██▌       [0m| 12/47 [00:15<00:07,  4.58it/s]r0 Training Epoch:  28%|[34m██▊       [0m| 13/47 [00:15<00:06,  5.10it/s]r0 Training Epoch:  30%|[34m██▉       [0m| 14/47 [00:15<00:05,  5.63it/s]r0 Training Epoch:  32%|[34m███▏      [0m| 15/47 [00:15<00:05,  5.98it/s]r0 Training Epoch:  34%|[34m███▍      [0m| 16/47 [00:15<00:04,  6.35it/s]r0 Training Epoch:  36%|[34m███▌      [0m| 17/47 [00:15<00:04,  6.61it/s]r0 Training Epoch:  38%|[34m███▊      [0m| 18/47 [00:15<00:04,  6.78it/s]r0 Training Epoch:  40%|[34m████      [0m| 19/47 [00:16<00:04,  6.89it/s]r0 Training Epoch:  43%|[34m████▎     [0m| 20/47 [00:16<00:03,  6.99it/s]r0 Training Epoch:  45%|[34m████▍     [0m| 21/47 [00:16<00:03,  7.07it/s]r0 Training Epoch:  47%|[34m████▋     [0m| 22/47 [00:16<00:03,  7.12it/s]r0 Training Epoch:  49%|[34m████▉     [0m| 23/47 [00:16<00:03,  7.15it/s]r0 Training Epoch:  51%|[34m█████     [0m| 24/47 [00:16<00:03,  7.19it/s]r0 Training Epoch:  53%|[34m█████▎    [0m| 25/47 [00:16<00:03,  7.20it/s]r0 Training Epoch:  55%|[34m█████▌    [0m| 26/47 [00:17<00:02,  7.19it/s]r0 Training Epoch:  57%|[34m█████▋    [0m| 27/47 [00:17<00:02,  7.20it/s]r0 Training Epoch:  60%|[34m█████▉    [0m| 28/47 [00:17<00:02,  7.12it/s]r0 Training Epoch:  62%|[34m██████▏   [0m| 29/47 [00:17<00:02,  7.19it/s]r0 Training Epoch:  64%|[34m██████▍   [0m| 30/47 [00:17<00:02,  7.25it/s]r0 Training Epoch:  66%|[34m██████▌   [0m| 31/47 [00:17<00:02,  7.14it/s]r0 Training Epoch:  68%|[34m██████▊   [0m| 32/47 [00:17<00:02,  7.21it/s]r0 Training Epoch:  70%|[34m███████   [0m| 33/47 [00:18<00:01,  7.21it/s]r0 Training Epoch:  72%|[34m███████▏  [0m| 34/47 [00:18<00:01,  7.10it/s]r0 Training Epoch:  74%|[34m███████▍  [0m| 35/47 [00:18<00:01,  7.18it/s]r0 Training Epoch:  77%|[34m███████▋  [0m| 36/47 [00:18<00:01,  7.22it/s]r0 Training Epoch:  79%|[34m███████▊  [0m| 37/47 [00:18<00:01,  7.27it/s]r0 Training Epoch:  81%|[34m████████  [0m| 38/47 [00:18<00:01,  7.24it/s]r0 Training Epoch:  83%|[34m████████▎ [0m| 39/47 [00:18<00:01,  7.30it/s]r0 Training Epoch:  85%|[34m████████▌ [0m| 40/47 [00:18<00:00,  7.33it/s]r0 Training Epoch:  87%|[34m████████▋ [0m| 41/47 [00:19<00:00,  7.28it/s]r0 Training Epoch:  89%|[34m████████▉ [0m| 42/47 [00:19<00:00,  7.29it/s]r0 Training Epoch:  91%|[34m█████████▏[0m| 43/47 [00:19<00:00,  7.32it/s]r0 Training Epoch:  94%|[34m█████████▎[0m| 44/47 [00:19<00:00,  7.37it/s]r0 Training Epoch:  96%|[34m█████████▌[0m| 45/47 [00:19<00:00,  7.39it/s]r0 Training Epoch:  98%|[34m█████████▊[0m| 46/47 [00:19<00:00,  7.42it/s]r0 Training Epoch: 100%|[34m██████████[0m| 47/47 [00:19<00:00,  7.46it/s]r0 Training Epoch: 100%|[34m██████████[0m| 47/47 [00:20<00:00,  2.35it/s]
Validation Epoch:   0%|[32m          [0m| 0/10 [00:00<?, ?it/s]Validation Epoch:  10%|[32m█         [0m| 1/10 [00:00<00:02,  3.82it/s]Validation Epoch:  40%|[32m████      [0m| 4/10 [00:00<00:00, 11.42it/s]Validation Epoch:  70%|[32m███████   [0m| 7/10 [00:00<00:00, 15.74it/s]Validation Epoch: 100%|[32m██████████[0m| 10/10 [00:00<00:00, 18.62it/s]Validation Epoch: 100%|[32m██████████[0m| 10/10 [00:00<00:00, 13.78it/s]
r0 Training Epoch:   0%|[34m          [0m| 0/47 [00:00<?, ?it/s]r0 Training Epoch:   2%|[34m▏         [0m| 1/47 [00:00<00:12,  3.76it/s]r0 Training Epoch:   4%|[34m▍         [0m| 2/47 [00:00<00:08,  5.23it/s]r0 Training Epoch:   6%|[34m▋         [0m| 3/47 [00:00<00:07,  6.03it/s]r0 Training Epoch:   9%|[34m▊         [0m| 4/47 [00:00<00:06,  6.49it/s]r0 Training Epoch:  11%|[34m█         [0m| 5/47 [00:00<00:06,  6.82it/s]r0 Training Epoch:  13%|[34m█▎        [0m| 6/47 [00:00<00:05,  6.99it/s]r0 Training Epoch:  15%|[34m█▍        [0m| 7/47 [00:01<00:05,  7.11it/s]r0 Training Epoch:  17%|[34m█▋        [0m| 8/47 [00:01<00:05,  7.21it/s]r0 Training Epoch:  19%|[34m█▉        [0m| 9/47 [00:01<00:05,  7.26it/s]r0 Training Epoch:  21%|[34m██▏       [0m| 10/47 [00:01<00:05,  7.25it/s]r0 Training Epoch:  23%|[34m██▎       [0m| 11/47 [00:01<00:04,  7.29it/s]r0 Training Epoch:  26%|[34m██▌       [0m| 12/47 [00:01<00:04,  7.33it/s]r0 Training Epoch:  28%|[34m██▊       [0m| 13/47 [00:01<00:04,  7.32it/s]r0 Training Epoch:  30%|[34m██▉       [0m| 14/47 [00:02<00:04,  7.37it/s]r0 Training Epoch:  32%|[34m███▏      [0m| 15/47 [00:02<00:04,  7.38it/s]r0 Training Epoch:  34%|[34m███▍      [0m| 16/47 [00:02<00:04,  7.37it/s]r0 Training Epoch:  36%|[34m███▌      [0m| 17/47 [00:02<00:04,  7.33it/s]r0 Training Epoch:  38%|[34m███▊      [0m| 18/47 [00:02<00:03,  7.35it/s]r0 Training Epoch:  40%|[34m████      [0m| 19/47 [00:02<00:03,  7.37it/s]r0 Training Epoch:  43%|[34m████▎     [0m| 20/47 [00:02<00:03,  7.38it/s]r0 Training Epoch:  45%|[34m████▍     [0m| 21/47 [00:02<00:03,  7.38it/s]r0 Training Epoch:  47%|[34m████▋     [0m| 22/47 [00:03<00:03,  7.39it/s]r0 Training Epoch:  49%|[34m████▉     [0m| 23/47 [00:03<00:03,  7.37it/s]r0 Training Epoch:  51%|[34m█████     [0m| 24/47 [00:03<00:03,  7.33it/s]r0 Training Epoch:  53%|[34m█████▎    [0m| 25/47 [00:03<00:03,  7.32it/s]r0 Training Epoch:  55%|[34m█████▌    [0m| 26/47 [00:03<00:02,  7.34it/s]r0 Training Epoch:  57%|[34m█████▋    [0m| 27/47 [00:03<00:02,  7.36it/s]r0 Training Epoch:  60%|[34m█████▉    [0m| 28/47 [00:03<00:02,  7.36it/s]r0 Training Epoch:  62%|[34m██████▏   [0m| 29/47 [00:04<00:02,  7.36it/s]r0 Training Epoch:  64%|[34m██████▍   [0m| 30/47 [00:04<00:02,  7.37it/s]r0 Training Epoch:  66%|[34m██████▌   [0m| 31/47 [00:04<00:02,  7.37it/s]r0 Training Epoch:  68%|[34m██████▊   [0m| 32/47 [00:04<00:02,  7.34it/s]r0 Training Epoch:  70%|[34m███████   [0m| 33/47 [00:04<00:01,  7.36it/s]r0 Training Epoch:  72%|[34m███████▏  [0m| 34/47 [00:04<00:01,  7.36it/s]r0 Training Epoch:  74%|[34m███████▍  [0m| 35/47 [00:04<00:01,  7.33it/s]r0 Training Epoch:  77%|[34m███████▋  [0m| 36/47 [00:05<00:01,  7.36it/s]r0 Training Epoch:  79%|[34m███████▊  [0m| 37/47 [00:05<00:01,  7.37it/s]r0 Training Epoch:  81%|[34m████████  [0m| 38/47 [00:05<00:01,  7.37it/s]r0 Training Epoch:  83%|[34m████████▎ [0m| 39/47 [00:05<00:01,  7.32it/s]r0 Training Epoch:  85%|[34m████████▌ [0m| 40/47 [00:05<00:00,  7.36it/s]r0 Training Epoch:  87%|[34m████████▋ [0m| 41/47 [00:05<00:00,  7.36it/s]r0 Training Epoch:  89%|[34m████████▉ [0m| 42/47 [00:05<00:00,  7.36it/s]r0 Training Epoch:  91%|[34m█████████▏[0m| 43/47 [00:05<00:00,  7.37it/s]r0 Training Epoch:  94%|[34m█████████▎[0m| 44/47 [00:06<00:00,  7.40it/s]r0 Training Epoch:  96%|[34m█████████▌[0m| 45/47 [00:06<00:00,  7.43it/s]r0 Training Epoch:  98%|[34m█████████▊[0m| 46/47 [00:06<00:00,  7.43it/s]r0 Training Epoch: 100%|[34m██████████[0m| 47/47 [00:06<00:00,  7.41it/s]r0 Training Epoch: 100%|[34m██████████[0m| 47/47 [00:06<00:00,  7.17it/s]
Validation Epoch:   0%|[32m          [0m| 0/10 [00:00<?, ?it/s]Validation Epoch:  10%|[32m█         [0m| 1/10 [00:00<00:01,  5.45it/s]Validation Epoch:  40%|[32m████      [0m| 4/10 [00:00<00:00, 14.28it/s]Validation Epoch:  70%|[32m███████   [0m| 7/10 [00:00<00:00, 18.05it/s]Validation Epoch: 100%|[32m██████████[0m| 10/10 [00:00<00:00, 20.31it/s]Validation Epoch: 100%|[32m██████████[0m| 10/10 [00:00<00:00, 16.00it/s]
