/home/hityangsir/parallel/MultiGPU_Training_Examples/pytorch_launch_per_node/torchrun/tutorial/ddp-tutorial-series/slurm
Thu Sep 21 11:13:23 EDT 2023
c0900a-s11.ufhpc
Node IP: 172.16.204.41
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : multigpu_torchrun.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 8
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
  role_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
  global_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
  role_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]
  global_world_sizes=[8, 8, 8, 8, 8, 8, 8, 8]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker2 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/2/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker3 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/3/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker4 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/4/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker5 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/5/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker6 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/6/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker7 reply file to: /scratch/local/10486428/torchelastic_9hotv2as/none_ciohjcv6/attempt_0/7/error.json
Loading snapshot
Loading snapshot
Resuming training from snapshot at Epoch 40Resuming training from snapshot at Epoch 40

Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
NCCL version 2.10.3+cuda11.3
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
[RANK0: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK1: local GPU1] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK2: local GPU2] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK5: local GPU5] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK3: local GPU3] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK4: local GPU4] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK6: local GPU6] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 40 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 41 | Batchsize: 32 | Steps: 8[RANK5: local GPU5] Epoch 41 | Batchsize: 32 | Steps: 8

[RANK3: local GPU3] Epoch 41 | Batchsize: 32 | Steps: 8
[RANK1: local GPU1] Epoch 41 | Batchsize: 32 | Steps: 8
[RANK4: local GPU4] Epoch 41 | Batchsize: 32 | Steps: 8
[RANK2: local GPU2] Epoch 41 | Batchsize: 32 | Steps: 8[RANK6: local GPU6] Epoch 41 | Batchsize: 32 | Steps: 8

Epoch 40 | Training snapshot saved at snapshot.pt
[RANK0: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 8
[RANK5: local GPU5] Epoch 42 | Batchsize: 32 | Steps: 8[RANK1: local GPU1] Epoch 42 | Batchsize: 32 | Steps: 8
[RANK3: local GPU3] Epoch 42 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 42 | Batchsize: 32 | Steps: 8

[RANK2: local GPU2] Epoch 42 | Batchsize: 32 | Steps: 8[RANK4: local GPU4] Epoch 42 | Batchsize: 32 | Steps: 8

[RANK6: local GPU6] Epoch 42 | Batchsize: 32 | Steps: 8
[RANK1: local GPU1] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK5: local GPU5] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK4: local GPU4] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK3: local GPU3] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK6: local GPU6] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK2: local GPU2] Epoch 43 | Batchsize: 32 | Steps: 8
[RANK1: local GPU1] Epoch 44 | Batchsize: 32 | Steps: 8[RANK5: local GPU5] Epoch 44 | Batchsize: 32 | Steps: 8

[RANK4: local GPU4] Epoch 44 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 44 | Batchsize: 32 | Steps: 8[RANK3: local GPU3] Epoch 44 | Batchsize: 32 | Steps: 8

[RANK2: local GPU2] Epoch 44 | Batchsize: 32 | Steps: 8
[RANK6: local GPU6] Epoch 44 | Batchsize: 32 | Steps: 8
[RANK1: local GPU1] Epoch 45 | Batchsize: 32 | Steps: 8
[RANK5: local GPU5] Epoch 45 | Batchsize: 32 | Steps: 8
[RANK4: local GPU4] Epoch 45 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 8
[RANK3: local GPU3] Epoch 45 | Batchsize: 32 | Steps: 8
[RANK2: local GPU2] Epoch 45 | Batchsize: 32 | Steps: 8[RANK6: local GPU6] Epoch 45 | Batchsize: 32 | Steps: 8

[RANK7: local GPU7] Epoch 45 | Batchsize: 32 | Steps: 8
[RANK5: local GPU5] Epoch 46 | Batchsize: 32 | Steps: 8[RANK1: local GPU1] Epoch 46 | Batchsize: 32 | Steps: 8

[RANK4: local GPU4] Epoch 46 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 46 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 8[RANK3: local GPU3] Epoch 46 | Batchsize: 32 | Steps: 8

[RANK2: local GPU2] Epoch 46 | Batchsize: 32 | Steps: 8
[RANK6: local GPU6] Epoch 46 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 8[RANK5: local GPU5] Epoch 47 | Batchsize: 32 | Steps: 8

[RANK1: local GPU1] Epoch 47 | Batchsize: 32 | Steps: 8[RANK4: local GPU4] Epoch 47 | Batchsize: 32 | Steps: 8

[RANK2: local GPU2] Epoch 47 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 47 | Batchsize: 32 | Steps: 8
[RANK3: local GPU3] Epoch 47 | Batchsize: 32 | Steps: 8
[RANK6: local GPU6] Epoch 47 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 8
[RANK5: local GPU5] Epoch 48 | Batchsize: 32 | Steps: 8
[RANK4: local GPU4] Epoch 48 | Batchsize: 32 | Steps: 8
[RANK1: local GPU1] Epoch 48 | Batchsize: 32 | Steps: 8[RANK2: local GPU2] Epoch 48 | Batchsize: 32 | Steps: 8

[RANK3: local GPU3] Epoch 48 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 48 | Batchsize: 32 | Steps: 8
[RANK6: local GPU6] Epoch 48 | Batchsize: 32 | Steps: 8
[RANK0: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 8
[RANK1: local GPU1] Epoch 49 | Batchsize: 32 | Steps: 8
[RANK5: local GPU5] Epoch 49 | Batchsize: 32 | Steps: 8
[RANK4: local GPU4] Epoch 49 | Batchsize: 32 | Steps: 8
[RANK7: local GPU7] Epoch 49 | Batchsize: 32 | Steps: 8[RANK3: local GPU3] Epoch 49 | Batchsize: 32 | Steps: 8[RANK2: local GPU2] Epoch 49 | Batchsize: 32 | Steps: 8


[RANK6: local GPU6] Epoch 49 | Batchsize: 32 | Steps: 8
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00031828880310058594 seconds
