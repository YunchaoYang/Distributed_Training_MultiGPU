Node IP: 172.16.205.13
172.16.205.13 192.168.130.193 192.168.130.194 192.168.130.195 192.168.130.196 10.13.141.13 192.168.130.197 192.168.130.198 192.168.130.199 192.168.130.200 10.13.141.14 10.13.173.13
Node IP: 172.16.205.13
/home/hityangsir/parallel/MultiGPU_Training_Examples/pytorch_launch_per_node/torchrun/tutorial/ddp-tutorial-series/slurm
c1007a-s17.ufhpc
Fri Mar 17 09:56:58 EDT 2023
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : multigpu_torchrun.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 28360
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.205.13:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /scratch/local/59752351/torchelastic_2f0k38zx/28360_mj1b671y
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : multigpu_torchrun.py
  min_nodes        : 2
  max_nodes        : 2
  nproc_per_node   : 2
  run_id           : 28360
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.205.13:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /scratch/local/59752351/torchelastic_44a4g0l0/28360_dg0t87bn
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=c1007a-s17.ufhpc
  master_port=56431
  group_rank=0
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /scratch/local/59752351/torchelastic_2f0k38zx/28360_mj1b671y/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /scratch/local/59752351/torchelastic_2f0k38zx/28360_mj1b671y/attempt_0/1/error.json
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=c1007a-s17.ufhpc
  master_port=56431
  group_rank=1
  group_world_size=2
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[4, 4]
  global_world_sizes=[4, 4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /scratch/local/59752351/torchelastic_44a4g0l0/28360_dg0t87bn/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /scratch/local/59752351/torchelastic_44a4g0l0/28360_dg0t87bn/attempt_0/1/error.json
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
[RANK0: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 40 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 40 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 41 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 41 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK2: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK0: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 42 | Batchsize: 32 | Steps: 16[RANK2: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16

[RANK2: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 47 | Batchsize: 32 | Steps: 16[RANK0: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16

[RANK2: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16[RANK3: local GPU1] Epoch 48 | Batchsize: 32 | Steps: 16

[RANK0: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 49 | Batchsize: 32 | Steps: 16
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.004529714584350586 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0005774497985839844 seconds
