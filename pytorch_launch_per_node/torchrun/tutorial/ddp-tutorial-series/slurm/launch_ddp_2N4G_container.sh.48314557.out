Node IP: 172.16.205.41
172.16.205.41 192.168.131.57 192.168.131.58 192.168.131.59 192.168.131.60 10.13.141.41 192.168.131.61 192.168.131.62 192.168.131.63 192.168.131.64 10.13.141.42 10.13.173.41 2620:104:1f:1001:63f:72ff:fec1:c979
Node IP: 172.16.205.41
/home/hityangsir/red/codebase/MultiGPU_Training_Examples/pytorch_launch_per_node/torchrun/tutorial/ddp-tutorial-series/slurm
c1100a-s11.ufhpc
Wed Oct 23 09:46:54 EDT 2024
INFO:    Environment variable SINGULARITY_DOCKER_PASSWORD is set, but APPTAINER_DOCKER_PASSWORD is preferred
INFO:    Environment variable SINGULARITY_DOCKER_USERNAME is set, but APPTAINER_DOCKER_USERNAME is preferred
INFO:    Environment variable SINGULARITY_DOCKER_PASSWORD is set, but APPTAINER_DOCKER_PASSWORD is preferred
INFO:    underlay of /etc/localtime required more than 50 (94) bind mounts
INFO:    Environment variable SINGULARITY_DOCKER_USERNAME is set, but APPTAINER_DOCKER_USERNAME is preferred
INFO:    underlay of /usr/bin/nvidia-cuda-mps-control required more than 50 (477) bind mounts
INFO:    underlay of /etc/localtime required more than 50 (94) bind mounts
INFO:    underlay of /usr/bin/nvidia-persistenced required more than 50 (477) bind mounts
/usr/bin/rm: cannot remove '/usr/local/cuda/compat/lib': Read-only file system
/usr/bin/rm: cannot remove '/usr/local/cuda/compat/lib': Read-only file system
rm: cannot remove '/usr/local/cuda/compat/lib': Read-only file system
rm: cannot remove '/usr/local/cuda/compat/lib': Read-only file system
W1023 09:48:07.411000 22736391873664 torch/distributed/run.py:778] 
W1023 09:48:07.411000 22736391873664 torch/distributed/run.py:778] *****************************************
W1023 09:48:07.411000 22736391873664 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1023 09:48:07.411000 22736391873664 torch/distributed/run.py:778] *****************************************
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   entrypoint       : multigpu_torchrun.py
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   min_nodes        : 2
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   max_nodes        : 2
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   nproc_per_node   : 2
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   run_id           : 28966
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : 172.16.205.41:29500
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}
W1023 09:48:07.411000 23063771333760 torch/distributed/run.py:778] 
W1023 09:48:07.411000 23063771333760 torch/distributed/run.py:778] *****************************************
W1023 09:48:07.411000 23063771333760 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1023 09:48:07.411000 23063771333760 torch/distributed/run.py:778] *****************************************
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   max_restarts     : 0
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   monitor_interval : 0.1
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   log_dir          : /scratch/local/48314557/torchelastic_yf2kacrf
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}
I1023 09:48:07.411000 22736391873664 torch/distributed/launcher/api.py:188] 
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   entrypoint       : multigpu_torchrun.py
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   min_nodes        : 2
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   max_nodes        : 2
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   nproc_per_node   : 2
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   run_id           : 28966
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : 172.16.205.41:29500
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   max_restarts     : 0
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   monitor_interval : 0.1
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   log_dir          : /scratch/local/48314557/torchelastic_i_gdsvdp
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}
I1023 09:48:07.412000 23063771333760 torch/distributed/launcher/api.py:188] 
I1023 09:48:07.603000 22736391873664 torch/distributed/elastic/agent/server/api.py:869] [default] starting workers for entrypoint: python
I1023 09:48:07.603000 22736391873664 torch/distributed/elastic/agent/server/api.py:702] [default] Rendezvous'ing worker group
I1023 09:48:08.531000 23063771333760 torch/distributed/elastic/agent/server/api.py:869] [default] starting workers for entrypoint: python
I1023 09:48:08.531000 23063771333760 torch/distributed/elastic/agent/server/api.py:702] [default] Rendezvous'ing worker group
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568] [default] Rendezvous complete for workers. Result:
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   restart_count=0
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   master_addr=c1100a-s11.ufhpc
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   master_port=50195
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   group_rank=0
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   group_world_size=2
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   local_ranks=[0, 1]
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   role_ranks=[0, 1]
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   global_ranks=[0, 1]
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   role_world_sizes=[4, 4]
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568]   global_world_sizes=[4, 4]
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:568] 
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/api.py:710] [default] Starting worker group
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568] [default] Rendezvous complete for workers. Result:
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   restart_count=0
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   master_addr=c1100a-s11.ufhpc
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   master_port=50195
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   group_rank=1
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   group_world_size=2
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   local_ranks=[0, 1]
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   role_ranks=[2, 3]
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   global_ranks=[2, 3]
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/local_elastic_agent.py:182] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   role_world_sizes=[4, 4]
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568]   global_world_sizes=[4, 4]
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:568] 
I1023 09:48:09.713000 22736391873664 torch/distributed/elastic/agent/server/local_elastic_agent.py:214] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/api.py:710] [default] Starting worker group
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/local_elastic_agent.py:182] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I1023 09:48:09.713000 23063771333760 torch/distributed/elastic/agent/server/local_elastic_agent.py:214] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
Loading snapshotLoading snapshot
Loading snapshot
Loading snapshot

Resuming training from snapshot at Epoch 40
Resuming training from snapshot at Epoch 40
Resuming training from snapshot at Epoch 40Resuming training from snapshot at Epoch 40

[RANK1: local GPU1] Epoch 40 | Batchsize: 32 | Steps: 16[RANK0: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16

[RANK2: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16[RANK3: local GPU1] Epoch 40 | Batchsize: 32 | Steps: 16

[RANK3: local GPU1] Epoch 41 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 41 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK0: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK2: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK3: local GPU1] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK1: local GPU1] Epoch 49 | Batchsize: 32 | Steps: 16
I1023 09:48:52.798000 23063771333760 torch/distributed/elastic/agent/server/api.py:888] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
I1023 09:48:52.798000 23063771333760 torch/distributed/elastic/agent/server/api.py:933] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
I1023 09:48:52.881000 22736391873664 torch/distributed/elastic/agent/server/api.py:888] [default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
I1023 09:48:52.881000 22736391873664 torch/distributed/elastic/agent/server/api.py:933] Local worker group finished (WorkerState.SUCCEEDED). Waiting 300 seconds for other agents to finish
I1023 09:48:52.882000 22736391873664 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.00011515617370605469 seconds
I1023 09:48:52.882000 23063771333760 torch/distributed/elastic/agent/server/api.py:946] Done waiting for other agents. Elapsed: 0.08384370803833008 seconds
