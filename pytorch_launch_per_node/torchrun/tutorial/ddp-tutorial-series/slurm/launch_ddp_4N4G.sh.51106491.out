Primary node: 
Primary TCP port: 
Secondary nodes: 
Node IP: 172.16.204.47
/home/hityangsir/parallel/MultiGPU_Training_Examples/pytorch_launch_per_node/torchrun/tutorial/ddp-tutorial-series/slurm
c0900a-s29.ufhpc
Mon Nov  7 20:44:22 EST 2022
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : multigpu_torchrun.py
  min_nodes        : 4
  max_nodes        : 4
  nproc_per_node   : 1
  run_id           : 29297
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.204.47:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : multigpu_torchrun.py
  min_nodes        : 4
  max_nodes        : 4
  nproc_per_node   : 1
  run_id           : 29297
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.204.47:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /scratch/local/51106491/torchelastic_5xovn4iy/29297_6lj7gxez
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : multigpu_torchrun.py
  min_nodes        : 4
  max_nodes        : 4
  nproc_per_node   : 1
  run_id           : 29297
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.204.47:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /scratch/local/51106491/torchelastic_by6daass/29297_wl4bzdwo
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : multigpu_torchrun.py
  min_nodes        : 4
  max_nodes        : 4
  nproc_per_node   : 1
  run_id           : 29297
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.204.47:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /scratch/local/51106491/torchelastic_mq9kmwtk/29297_yg07tzet
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /scratch/local/51106491/torchelastic_3preo7ni/29297_qftgr65j
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=c0900a-s29.ufhpc
  master_port=38177
  group_rank=0
  group_world_size=4
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[4]
  global_world_sizes=[4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /scratch/local/51106491/torchelastic_5xovn4iy/29297_6lj7gxez/attempt_0/0/error.json
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=c0900a-s29.ufhpc
  master_port=38177
  group_rank=3
  group_world_size=4
  local_ranks=[0]
  role_ranks=[3]
  global_ranks=[3]
  role_world_sizes=[4]
  global_world_sizes=[4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=c0900a-s29.ufhpc
  master_port=38177
  group_rank=1
  group_world_size=4
  local_ranks=[0]
  role_ranks=[1]
  global_ranks=[1]
  role_world_sizes=[4]
  global_world_sizes=[4]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=c0900a-s29.ufhpc
  master_port=38177
  group_rank=2
  group_world_size=4
  local_ranks=[0]
  role_ranks=[2]
  global_ranks=[2]
  role_world_sizes=[4]
  global_world_sizes=[4]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /scratch/local/51106491/torchelastic_mq9kmwtk/29297_yg07tzet/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /scratch/local/51106491/torchelastic_by6daass/29297_wl4bzdwo/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /scratch/local/51106491/torchelastic_3preo7ni/29297_qftgr65j/attempt_0/0/error.json
Loading snapshot
Resuming training from snapshot at Epoch 40
NCCL version 2.10.3+cuda11.3
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
Loading snapshot
Resuming training from snapshot at Epoch 40
[RANK0: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 40 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK0: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK3: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK1: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
Epoch 40 | Training snapshot saved at snapshot.pt
[RANK2: local GPU0] Epoch 41 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 42 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 43 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 44 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 45 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 46 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 47 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 48 | Batchsize: 32 | Steps: 16
[RANK0: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK1: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK3: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
[RANK2: local GPU0] Epoch 49 | Batchsize: 32 | Steps: 16
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.007019519805908203 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.002135753631591797 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0049059391021728516 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0014045238494873047 seconds
